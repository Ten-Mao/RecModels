{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        try:\n",
    "            yield eval(l)\n",
    "        except:\n",
    "            yield json.loads(l)\n",
    "        \n",
    "'''\n",
    "Set seeds\n",
    "'''\n",
    "seed = 2022\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amazon2014(dataset_name, min_rating_score, data_core_num=None):\n",
    "    '''\n",
    "    reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "    asin - ID of the product, e.g. 0000013714\n",
    "    reviewerName - name of the reviewer\n",
    "    helpful - helpfulness rating of the review, e.g. 2/3\n",
    "    --\"helpful\": [2, 3],\n",
    "    reviewText - text of the review\n",
    "    --\"reviewText\": \"I bought this for my husband who plays the piano. ...\"\n",
    "    overall - rating of the product\n",
    "    --\"overall\": 5.0,\n",
    "    summary - summary of the review\n",
    "    --\"summary\": \"Heavenly Highway Hymns\",\n",
    "    unixReviewTime - time of the review (unix time)\n",
    "    --\"unixReviewTime\": 1252800000,\n",
    "    reviewTime - time of the review (raw)\n",
    "    --\"reviewTime\": \"09 13, 2009\"\n",
    "    '''\n",
    "    datas = []\n",
    "\n",
    "    if data_core_num is not None:\n",
    "        data_file = f\"./{dataset_name}/raw/reviews_{dataset_name}_{data_core_num}.json.gz\"\n",
    "    else:\n",
    "        data_file = f\"./{dataset_name}/raw/reviews_{dataset_name}.json.gz\"\n",
    "\n",
    "    for inter in parse(data_file):\n",
    "        if float(inter['overall']) <= min_rating_score: # 小于一定分数去掉\n",
    "            continue\n",
    "        user = inter['reviewerID']\n",
    "        item = inter['asin']\n",
    "        time = inter['unixReviewTime']\n",
    "        datas.append((user, item, int(time)))\n",
    "    \n",
    "    return datas\n",
    "\n",
    "def Amazon2014_item_meta(dataset_name, data_maps):\n",
    "    '''\n",
    "    asin - ID of the product, e.g. 0000031852\n",
    "    --\"asin\": \"0000031852\",\n",
    "    title - name of the product\n",
    "    --\"title\": \"Girls Ballet Tutu Zebra Hot Pink\",\n",
    "    description\n",
    "    price - price in US dollars (at time of crawl)\n",
    "    --\"price\": 3.17,\n",
    "    imUrl - url of the product image (str)\n",
    "    --\"imUrl\": \"http://ecx.images-amazon.com/images/I/51fAmVkTbyL._SY300_.jpg\",\n",
    "    related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "    --\"related\":{\n",
    "        \"also_bought\": [\"B00JHONN1S\"],\n",
    "        \"also_viewed\": [\"B002BZX8Z6\"],\n",
    "        \"bought_together\": [\"B002BZX8Z6\"]\n",
    "    },\n",
    "    salesRank - sales rank information\n",
    "    --\"salesRank\": {\"Toys & Games\": 211836}\n",
    "    brand - brand name\n",
    "    --\"brand\": \"Coxlures\",\n",
    "    categories - list of categories the product belongs to\n",
    "    --\"categories\": [[\"Sports & Outdoors\", \"Other Sports\", \"Dance\"]]\n",
    "    '''\n",
    "    datas = {}\n",
    "    meta_file = f\"./{dataset_name}/raw/meta_{dataset_name}.json.gz\"\n",
    "    item_asins = list(data_maps['item2id'].keys())\n",
    "    for info in parse(meta_file):\n",
    "        if info['asin'] not in item_asins:\n",
    "            continue\n",
    "        datas[info['asin']] = info\n",
    "    return datas\n",
    "\n",
    "def Amazon2014_user_meta(dataset_name, data_maps):\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def Yelp(date_min, date_max, rating_score):\n",
    "    datas = []\n",
    "    data_flie = './Yelp/raw/yelp_academic_dataset_review.json'\n",
    "    lines = open(data_flie).readlines()\n",
    "    for line in tqdm(lines):\n",
    "        review = json.loads(line.strip())\n",
    "        user = review['user_id']\n",
    "        item = review['business_id']\n",
    "        rating = review['stars']\n",
    "        date = review['date']\n",
    "        if date < date_min or date > date_max or float(rating) <= rating_score:\n",
    "            continue\n",
    "\n",
    "        # 将date转换为直接可比的数字串 2004-10-12 10:13:32 -> 20041012101332\n",
    "        time = date.replace('-','').replace(':','').replace(' ','')\n",
    "        datas.append((user, item, int(time)))\n",
    "    return datas\n",
    "\n",
    "def Yelp_item_meta(datamaps):\n",
    "    meta_infos = {}\n",
    "    meta_file = './Yelp/raw/yelp_academic_dataset_business.json'\n",
    "    item_ids = list(datamaps['item2id'].keys())\n",
    "    lines = open(meta_file).readlines()\n",
    "    for line in tqdm(lines):\n",
    "        info = json.loads(line)\n",
    "        if info['business_id'] not in item_ids:\n",
    "            continue\n",
    "        meta_infos[info['business_id']] = info\n",
    "    return meta_infos\n",
    "\n",
    "def Yelp_user_meta(data_maps):\n",
    "    meta_infos = {}\n",
    "    meta_file = './Yelp/raw/yelp_academic_dataset_user.json'\n",
    "    user_ids = list(data_maps['user2id'].keys())\n",
    "    lines = open(meta_file).readlines()\n",
    "    for line in tqdm(lines):\n",
    "        info = json.loads(line)\n",
    "        if info['user_id'] not in user_ids:\n",
    "            continue\n",
    "        meta_infos[info['user_id']] = info\n",
    "    return meta_infos\n",
    "\n",
    "# 获取inter字典\n",
    "def get_interaction(datas):\n",
    "    user_seq = {}\n",
    "\n",
    "    # user_seq[user] = [item1, item2, item3, ...]\n",
    "    for data in datas:\n",
    "        user, item, time = data\n",
    "        if user in user_seq:\n",
    "            user_seq[user].append((item, time))\n",
    "        else:\n",
    "            user_seq[user] = []\n",
    "            user_seq[user].append((item, time))\n",
    "\n",
    "    # 按照时间排序\n",
    "    for user, item_time in user_seq.items():\n",
    "        item_time.sort(key=lambda x: x[1])\n",
    "        items = []\n",
    "        for t in item_time:\n",
    "            items.append(t[0])\n",
    "        user_seq[user] = items\n",
    "    return user_seq\n",
    "\n",
    "# 检查是否满足K-core\n",
    "def check_Kcore(user_items, user_core, item_core):\n",
    "    user_count = defaultdict(int)\n",
    "    item_count = defaultdict(int)\n",
    "    for user, items in user_items.items():\n",
    "        for item in items:\n",
    "            user_count[user] += 1\n",
    "            item_count[item] += 1\n",
    "\n",
    "    for user, num in user_count.items():\n",
    "        if num < user_core:\n",
    "            return user_count, item_count, False\n",
    "    for item, num in item_count.items():\n",
    "        if num < item_core:\n",
    "            return user_count, item_count, False\n",
    "    return user_count, item_count, True # 已经保证Kcore\n",
    "\n",
    "# K-core 过滤\n",
    "def filter_Kcore(user_items, user_core, item_core):\n",
    "    user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    while not isKcore:\n",
    "        for user, num in user_count.items():\n",
    "            if user_count[user] < user_core: # 直接把user 删除\n",
    "                user_items.pop(user)\n",
    "            else:\n",
    "                user_items[user] = list(filter(lambda item: item_count[item] >= item_core, user_items[user]))\n",
    "        user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    return user_items\n",
    "\n",
    "# 重新映射id\n",
    "def id_map(user_items): # user_items dict\n",
    "    user2id = {} # raw 2 uid\n",
    "    item2id = {} # raw 2 iid\n",
    "    id2user = {} # uid 2 raw\n",
    "    id2item = {} # iid 2 raw\n",
    "    user_id = 0\n",
    "    item_id = 0\n",
    "    final_data = {}\n",
    "    random_user_list = list(user_items.keys())\n",
    "    random.shuffle(random_user_list)\n",
    "    for user in random_user_list:\n",
    "        items = user_items[user]\n",
    "        if user not in user2id:\n",
    "            user2id[user] = str(user_id)\n",
    "            id2user[str(user_id)] = user\n",
    "            user_id += 1\n",
    "        iids = [] # item id lists\n",
    "        for item in items:\n",
    "            if item not in item2id:\n",
    "                item2id[item] = str(item_id)\n",
    "                id2item[str(item_id)] = item\n",
    "                item_id += 1\n",
    "            iids.append(item2id[item])\n",
    "        uid = user2id[user]\n",
    "        final_data[uid] = iids\n",
    "    data_maps = {\n",
    "        'user2id': user2id,\n",
    "        'item2id': item2id,\n",
    "        'id2user': id2user,\n",
    "        'id2item': id2item\n",
    "    }\n",
    "    return final_data, user_id, item_id, data_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def main(dataset_name, data_type:Literal[\"Amazon2014\", \"Yelp\"]=\"Amazon2014\"):\n",
    "\n",
    "    # 设置最低评分以及用户和物品的最低交互次数\n",
    "    min_rating_score = 0.0\n",
    "    user_core = 5\n",
    "    item_core = 5\n",
    "\n",
    "    # 读取raw数据\n",
    "    if data_type == 'Yelp':\n",
    "        date_max = '2019-12-31 00:00:00'\n",
    "        date_min = '2019-01-01 00:00:00'\n",
    "        datas = Yelp(date_min, date_max, min_rating_score)\n",
    "    elif data_type == \"Amazon2014\":\n",
    "        datas = Amazon2014(dataset_name, min_rating_score=min_rating_score, data_core_num=5)\n",
    "    else:\n",
    "        raise ValueError(f\"Data type {data_type} is not supported!\")\n",
    "\n",
    "    # 获取inter信息\n",
    "    user_items = get_interaction(datas)\n",
    "    print(f'{dataset_name} Raw data has been processed! Lower than {min_rating_score} are deleted!')\n",
    "    \n",
    "    # 进行K-core过滤\n",
    "    user_items = filter_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    print(f'User {user_core}-core complete! Item {item_core}-core complete!')\n",
    "\n",
    "    # 重新映射id\n",
    "    user_items, user_num, item_num, data_maps = id_map(user_items)\n",
    "\n",
    "    # 记录数据集的基本信息：用户数、物品数、交互数、稀疏度等\n",
    "    user_count, item_count, _ = check_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    user_count_list = list(user_count.values())\n",
    "    user_avg, user_min, user_max = np.mean(user_count_list), np.min(user_count_list), np.max(user_count_list)\n",
    "    user_std = np.std(user_count_list)\n",
    "    item_count_list = list(item_count.values())\n",
    "    item_avg, item_min, item_max = np.mean(item_count_list), np.min(item_count_list), np.max(item_count_list)\n",
    "    item_std = np.std(item_count_list)\n",
    "    interact_num = np.sum(user_count_list)\n",
    "    sparsity = (1 - interact_num / (user_num * item_num)) * 100\n",
    "    show_info = f'Total User: {user_num}, Avg User: {user_avg:.4f}, Std User: {user_std: .4f}, Min Len: {user_min}, Max Len: {user_max}\\n' + \\\n",
    "                f'Total Item: {item_num}, Avg Item: {item_avg:.4f}, Std Item: {item_std: .4f}, Min Inter: {item_min}, Max Inter: {item_max}\\n' + \\\n",
    "                f'Iteraction Num: {interact_num}, Sparsity: {sparsity:.2f}%'\n",
    "    print(show_info)\n",
    "\n",
    "\n",
    "    print('\\nBegin extracting items meta infos...')\n",
    "    if data_type == 'Amazon2014':\n",
    "        item_meta_infos = Amazon2014_item_meta(dataset_name, data_maps)\n",
    "        # attribute_num, avg_attribute, datamaps, item2attributes = get_attribute_Amazon(meta_infos, data_maps, attribute_core)\n",
    "    elif data_type == 'Yelp':\n",
    "        item_meta_infos = Yelp_item_meta(data_maps)\n",
    "        # attribute_num, avg_attribute, datamaps, item2attributes = get_attribute_Yelp(meta_infos, data_maps, attribute_core)\n",
    "    else:\n",
    "        raise ValueError(f\"Data type {data_type} is not supported!\")\n",
    "    print('Items meta infos extracted complete!')\n",
    "\n",
    "\n",
    "    print('\\nBegin extracting users meta infos...')\n",
    "    if data_type == 'Amazon2014':\n",
    "        user_meta_infos = Amazon2014_user_meta(dataset_name, data_maps)\n",
    "    elif data_type == 'Yelp':\n",
    "        user_meta_infos = Yelp_user_meta(data_maps)\n",
    "    else:\n",
    "        raise ValueError(f\"Data type {data_type} is not supported!\")\n",
    "    print('Users meta infos extracted complete!')\n",
    "\n",
    "\n",
    "    print('\\nBegin saving data...')\n",
    "    # -------------- Save Data ---------------\n",
    "    inter_file = f'{dataset_name}/{dataset_name}.inter.csv'\n",
    "    item_file = f'{dataset_name}/{dataset_name}.item.csv'\n",
    "    user_file = f'{dataset_name}/{dataset_name}.user.csv'\n",
    "\n",
    "    # 保存交互数据\n",
    "    with open(f'{data_type}/{data_type}.inter.json', 'w') as f:\n",
    "        json.dump(user_items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 保存物品数据\n",
    "    items = {}\n",
    "    for iid, item in data_maps['id2item'].items():\n",
    "        info = item_meta_infos[item]\n",
    "        if dataset_name == \"Beauty2014\":\n",
    "            title = info.get('title', f\"title_{iid}\")\n",
    "            description = info.get('description', f\"description_{iid}\")\n",
    "        elif dataset_name == \"Yelp\":\n",
    "            title = info.get('name', f\"title_{iid}\")\n",
    "            description = info.get('categories', f\"description_{iid}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset {dataset_name} is not supported!\")\n",
    "        items[iid] = {\n",
    "            \"title\": title,\n",
    "            \"description\": description\n",
    "        }\n",
    "    with open(f'{data_type}/{data_type}.item.json', 'w') as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # # 保存用户数据\n",
    "    # if dataset_name == \"Beauty2014\":\n",
    "    #     users = []\n",
    "    #     for uid, user in data_maps['id2user'].items():\n",
    "    #         users.append([uid])\n",
    "    #     user_df = pd.DataFrame(users, columns=['user_id'])\n",
    "    #     user_df.to_csv(user_file, index=False, sep=',', quoting=1, encoding='utf-8')\n",
    "    # elif dataset_name == \"Yelp\":\n",
    "    #     users = []\n",
    "    #     for uid, user in data_maps['id2user'].items():\n",
    "    #         info = user_meta_infos[user]\n",
    "    #         name = info.get('name', f\"name_{uid}\")\n",
    "    #         users.append([uid, name])\n",
    "    #     user_df = pd.DataFrame(users, columns=['user_id', 'name'])\n",
    "    #     user_df.to_csv(user_file, index=False, sep=',', quoting=1, encoding='utf-8')\n",
    "\n",
    "    print('Data saved complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beauty2014 Raw data has been processed! Lower than 0.0 are deleted!\n",
      "User 5-core complete! Item 5-core complete!\n",
      "Total User: 22363, Avg User: 8.8764, Std User:  8.1636, Min Len: 5, Max Len: 204\n",
      "Total Item: 12101, Avg Item: 16.4038, Std Item:  23.6090, Min Inter: 5, Max Inter: 431\n",
      "Iteraction Num: 198502, Sparsity: 99.93%\n",
      "\n",
      "Begin extracting items meta infos...\n",
      "Items meta infos extracted complete!\n",
      "\n",
      "Begin extracting users meta infos...\n",
      "Users meta infos extracted complete!\n",
      "\n",
      "Begin saving data...\n",
      "Data saved complete!\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Beauty2014\"\n",
    "\n",
    "main(dataset_name, data_type='Amazon2014')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
