{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 转化为 json\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "items = {}\n",
    "item_df = pd.read_csv('Beauty2014/Beauty2014.item.csv')\n",
    "for row in item_df.itertuples():\n",
    "    item_id = getattr(row, \"item_id\")\n",
    "    title = getattr(row, \"title\")\n",
    "    discription = getattr(row, \"description\")\n",
    "    title = \"\" if pd.isnull(title) else title\n",
    "    discription = \"\" if pd.isnull(discription) else discription\n",
    "    items[item_id] = {\"title\": title, \"discription\": discription}\n",
    "\n",
    "with open('Beauty2014/Beauty2014.item.json', 'w') as f:\n",
    "    json.dump(items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "inters = {}\n",
    "inter_df = pd.read_csv(\"Beauty2014/Beauty2014.inter.csv\")\n",
    "for row in inter_df.itertuples():\n",
    "    user_id = getattr(row, \"user_id\")\n",
    "    item_id = getattr(row, \"item_id\")\n",
    "    if user_id not in inters:\n",
    "        inters[user_id] = []\n",
    "    inters[user_id].append(item_id)\n",
    "\n",
    "with open('Beauty2014/Beauty2014.inter.json', 'w') as f:\n",
    "    json.dump(inters, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数据集的信息\n",
    "import json\n",
    "\n",
    "inters = json.load(open(\"Beauty2014/Beauty2014.inter.json\", \"r\"))\n",
    "\n",
    "user_count = {}\n",
    "item_count = {}\n",
    "\n",
    "for user_id, items in inters.items():\n",
    "    for item_id in items:\n",
    "        if item_id not in item_count:\n",
    "            item_count[item_id] = 0\n",
    "        item_count[item_id] += 1\n",
    "    user_count[user_id] = len(items)\n",
    "\n",
    "train_target_count = []\n",
    "valid_target_count = []\n",
    "test_target_count = []\n",
    "\n",
    "for user_id, items in inters.items():\n",
    "    for index in range(1, len(items)-2):\n",
    "        train_target_count.append(item_count[items[index]])\n",
    "    valid_target_count.append(item_count[items[-2]])\n",
    "    test_target_count.append(item_count[items[-1]])\n",
    "print(f\"item_avg_count: {sum(item_count.values())/len(item_count)}\")\n",
    "print(f\"train_target_count: {sum(train_target_count)/len(train_target_count)}\")\n",
    "print(f\"valid_target_count: {sum(valid_target_count)/len(valid_target_count)}\")\n",
    "print(f\"test_target_count: {sum(test_target_count)/len(test_target_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将sasrec.pth 转化为 pt\n",
    "import torch\n",
    "import os\n",
    "path = \"\" # dir path\n",
    "os.chdir(path)  # 切换目录\n",
    "sasrec = torch.load(\"save/Beauty2014/SASRec-d_model_32-train_batch_size_256-lr_0.01-wd_0.01.pth\", weights_only=False)\n",
    "emb = sasrec.item_emb.weight\n",
    "torch.save(emb, \"data/Beauty2014/Beauty2014-cf_emb.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.3159e-04, 4.3609e-05, 1.4472e-05, 4.7719e-06], device='cuda:0')\n",
      "tensor([9.8870e-01, 2.9377e-03, 2.6375e-04, 6.3479e-05], device='cuda:0')\n",
      "tensor([4.0028e-03, 9.8898e-01, 4.8369e-04, 9.3770e-05], device='cuda:0')\n",
      "tensor([4.2089e-03, 4.8651e-03, 9.9879e-01, 2.0790e-04], device='cuda:0')\n",
      "tensor([2.8552e-03, 3.1775e-03, 4.4549e-04, 9.9963e-01], device='cuda:0')\n",
      "tensor([[0.2872],\n",
      "        [0.1807],\n",
      "        [0.7915],\n",
      "        [0.9398]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "path = \"/home/liuchang/RecModels\" # dir path\n",
    "os.chdir(path)  # 切换目录\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import SeqRecDataset\n",
    "import torch.nn as nn\n",
    "def prepare_input(\n",
    "    his_seqs,\n",
    "    next_items,\n",
    "    indice_matrix,\n",
    "):\n",
    "    # his_seqs [batch_size, max_len]\n",
    "    # next_items [batch_size]\n",
    "\n",
    "    encoder_input_ids = indice_matrix(his_seqs) # [batch_size, max_len, codebook_num]\n",
    "    decoder_input_ids = indice_matrix(next_items) # [batch_size, codebook_num]\n",
    "\n",
    "    # rescale his_seqs and next_items\n",
    "    encoder_shift = (\n",
    "        torch.tensor([2 + i * 256 for i in range(encoder_input_ids.shape[-1])])\n",
    "        .reshape(1, 1, encoder_input_ids.shape[-1])\n",
    "        .to(encoder_input_ids.device)\n",
    "    )\n",
    "    shift = decoder_shift = (\n",
    "        torch.tensor([2 + i * 256 for i in range(decoder_input_ids.shape[-1])])\n",
    "        .reshape(1, decoder_input_ids.shape[-1])\n",
    "        .to(decoder_input_ids.device)\n",
    "    )\n",
    "    encoder_input_ids = (encoder_input_ids + encoder_shift).long() # [batch_size, max_len, codebook_num]\n",
    "    decoder_input_ids = (decoder_input_ids + decoder_shift).long() # [batch_size, codebook_num]\n",
    "\n",
    "\n",
    "    # write pad to encoder_input_ids\n",
    "    pad_indices = (his_seqs == 0).long().nonzero(as_tuple=True)\n",
    "    encoder_input_ids[pad_indices[0], pad_indices[1]] = torch.zeros(encoder_input_ids.shape[-1], dtype=torch.long).to(encoder_input_ids.device)\n",
    "    encoder_input_ids = encoder_input_ids.reshape(encoder_input_ids.shape[0], -1) # [batch_size, max_len*codebook_num]\n",
    "\n",
    "    # add eos to encoder_input_ids\n",
    "    encoder_input_ids = torch.cat(\n",
    "        [\n",
    "            encoder_input_ids, \n",
    "            torch.ones((encoder_input_ids.shape[0], 1), dtype=torch.long).to(encoder_input_ids.device)\n",
    "        ], \n",
    "        dim=-1\n",
    "    ) # [batch_size, max_len*codebook_num + 1]\n",
    "\n",
    "    # add eos to labels\n",
    "    labels = torch.cat(\n",
    "        [\n",
    "            decoder_input_ids,\n",
    "            torch.ones((decoder_input_ids.shape[0], 1), dtype=torch.long).to(encoder_input_ids.device),\n",
    "        ],\n",
    "        dim=-1\n",
    "    ) # [batch_size, codebook_num + 1]\n",
    "\n",
    "    # add decoder_input_token to decoder_input_ids\n",
    "    decoder_input_ids = torch.cat(\n",
    "        [\n",
    "            torch.zeros((decoder_input_ids.shape[0], 1), dtype=torch.long).to(encoder_input_ids.device),\n",
    "            decoder_input_ids\n",
    "        ],\n",
    "        dim=-1\n",
    "    ) # [batch_size, 1 + codebook_num]\n",
    "\n",
    "    encoder_mask = (encoder_input_ids != 0).int() # [batch_size, max_len*codebook_num + 1]\n",
    "\n",
    "    return encoder_input_ids, encoder_mask, decoder_input_ids, labels, shift\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"./data\"\n",
    "dataset_name = \"Beauty2014\"\n",
    "max_len = 20\n",
    "seed = 1024\n",
    "num_workers = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rqvae = torch.load(\"save/Beauty2014/TIGER/rqvae-tiger-lr_0.001-wd_0.0001-best.pth\", weights_only=False).to(device)\n",
    "t54rec = torch.load(\"save/Beauty2014/TIGER/TIGER-rqvae_lr_0.001-rqvae_wd_0.0001-rqvae_select_position_best-t54rec_lr_0.0005-t54rec_wd_0.01-t54rec_epochs_200-scheduler_type_cosine-warmup_ratio_0.01-t54rec_train_batch_size_256-t54rec_early_stop_step_20-beam_size_20.pth\", weights_only=False).to(device)\n",
    "rqvae.update_labels()\n",
    "rqvae_indices = rqvae.get_all_indices()\n",
    "indice_matrix = nn.Embedding(rqvae.item_emb.weight.shape[0], 4).to(device)\n",
    "indice_matrix.weight.data.copy_(rqvae_indices)\n",
    "codebook1_labels_list = []\n",
    "codebook2_labels_list = []\n",
    "codebook3_labels_list = []\n",
    "codebook4_labels_list = []\n",
    "for idx, vq in enumerate(rqvae.rq.codebooks):\n",
    "    labels = vq.labels\n",
    "    for i in range(10):\n",
    "        indices = [j for j in range(len(labels)) if labels[j] == i]\n",
    "        if idx == 0:\n",
    "            codebook1_labels_list.append([j + 2 + idx * 256 for j in indices])\n",
    "        elif idx == 1:\n",
    "            codebook2_labels_list.append([j + 2 + idx * 256 for j in indices])\n",
    "        elif idx == 2:\n",
    "            codebook3_labels_list.append([j + 2 + idx * 256 for j in indices])\n",
    "        elif idx == 3:\n",
    "            codebook4_labels_list.append([j + 2 + idx * 256 for j in indices])\n",
    "\n",
    "dataset = SeqRecDataset(\n",
    "    data_root_path=data_path, \n",
    "    dataset=dataset_name, \n",
    "    max_len=max_len, \n",
    "    mode=\"test\", \n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "all_probs_spe = []\n",
    "all_probs_codebook1 = []\n",
    "all_probs_codebook2 = []\n",
    "all_probs_codebook3 = []\n",
    "all_probs_codebook4 = []\n",
    "all_probs_codebook_cluster = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        his_seqs = batch[\"his_seqs\"]\n",
    "        next_items = batch[\"next_items\"]\n",
    "        (\n",
    "            encoder_input_ids, \n",
    "            encoder_mask, \n",
    "            decoder_input_ids, \n",
    "            labels,\n",
    "            shift,\n",
    "        ) = prepare_input(\n",
    "            his_seqs,\n",
    "            next_items,\n",
    "            indice_matrix\n",
    "        )\n",
    "\n",
    "        logits = t54rec._forward(\n",
    "            input_ids=encoder_input_ids,\n",
    "            attention_mask=encoder_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "            return_dict=False\n",
    "        )[1][:, :-1, :] # [batch_size, 4, vocab_size]\n",
    "        target_cluster = []\n",
    "        for i in range(logits.shape[0]):\n",
    "            clusters = []\n",
    "            for j in range(4):\n",
    "                clusters.append(rqvae.rq.codebooks[j].labels[decoder_input_ids[i, j + 1].cpu().numpy() - 2 - j * 256])\n",
    "            target_cluster.append(torch.tensor(clusters))\n",
    "        target_cluster = torch.stack(target_cluster, dim=0).unsqueeze(-1) # [batch_size, 4, 1]\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1) # [batch_size, 4, vocab_size]\n",
    "\n",
    "        # 码表间概率分布\n",
    "        probs_spe = probs[:, :, :2].sum(dim=-1) # [batch_size, 4]\n",
    "        probs_codebook1 = probs[:, :, 2: 258].sum(dim=-1) # [batch_size, 4]\n",
    "        probs_codebook2 = probs[:, :, 258: 514].sum(dim=-1) # [batch_size, 4]\n",
    "        probs_codebook3 = probs[:, :, 514: 770].sum(dim=-1) # [batch_size, 4]\n",
    "        probs_codebook4 = probs[:, :, 770:].sum(dim=-1) # [batch_size, 4]\n",
    "\n",
    "        # 码表内聚簇概率分布\n",
    "        probs_codebook_cluster = []\n",
    "        for i in range(probs.shape[0]):\n",
    "            probs_codebook1_cluster_temp = []\n",
    "            probs_codebook2_cluster_temp = []\n",
    "            probs_codebook3_cluster_temp = []\n",
    "            probs_codebook4_cluster_temp = []\n",
    "            for j in range(4):\n",
    "                if j == 0:\n",
    "                    for k in range(10):\n",
    "                        probs_codebook1_cluster_temp.append(probs[i, j, codebook1_labels_list[k]].sum())\n",
    "                elif j == 1:\n",
    "                    for k in range(10):\n",
    "                        probs_codebook2_cluster_temp.append(probs[i, j, codebook2_labels_list[k]].sum())\n",
    "                elif j == 2:\n",
    "                    for k in range(10):\n",
    "                        probs_codebook3_cluster_temp.append(probs[i, j, codebook3_labels_list[k]].sum())\n",
    "                elif j == 3:\n",
    "                    for k in range(10):\n",
    "                        probs_codebook4_cluster_temp.append(probs[i, j, codebook4_labels_list[k]].sum())\n",
    "            probs_codebook1_cluster_temp = torch.tensor(probs_codebook1_cluster_temp).reshape(1, 10)\n",
    "            probs_codebook2_cluster_temp = torch.tensor(probs_codebook2_cluster_temp).reshape(1, 10)\n",
    "            probs_codebook3_cluster_temp = torch.tensor(probs_codebook3_cluster_temp).reshape(1, 10)\n",
    "            probs_codebook4_cluster_temp = torch.tensor(probs_codebook4_cluster_temp).reshape(1, 10)\n",
    "            probs_codebook_cluster.append(torch.cat([probs_codebook1_cluster_temp, probs_codebook2_cluster_temp, probs_codebook3_cluster_temp, probs_codebook4_cluster_temp], dim=0))\n",
    "        probs_codebook_cluster = torch.stack(probs_codebook_cluster, dim=0).gather(-1, target_cluster) # [batch_size, 4]\n",
    "\n",
    "        all_probs_spe.append(probs_spe)\n",
    "        all_probs_codebook1.append(probs_codebook1)\n",
    "        all_probs_codebook2.append(probs_codebook2)\n",
    "        all_probs_codebook3.append(probs_codebook3)\n",
    "        all_probs_codebook4.append(probs_codebook4)\n",
    "        all_probs_codebook_cluster.append(probs_codebook_cluster)\n",
    "    all_probs_spe = torch.cat(all_probs_spe, dim=0).mean(dim=0)\n",
    "    all_probs_codebook1 = torch.cat(all_probs_codebook1, dim=0).mean(dim=0)\n",
    "    all_probs_codebook2 = torch.cat(all_probs_codebook2, dim=0).mean(dim=0)\n",
    "    all_probs_codebook3 = torch.cat(all_probs_codebook3, dim=0).mean(dim=0)\n",
    "    all_probs_codebook4 = torch.cat(all_probs_codebook4, dim=0).mean(dim=0)\n",
    "    all_probs_codebook_cluster = torch.cat(all_probs_codebook_cluster, dim=0).mean(dim=0)\n",
    "    print(all_probs_spe)\n",
    "    print(all_probs_codebook1)\n",
    "    print(all_probs_codebook2)\n",
    "    print(all_probs_codebook3)\n",
    "    print(all_probs_codebook4)\n",
    "    print(all_probs_codebook_cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
